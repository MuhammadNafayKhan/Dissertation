{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cryptocurrency Forecasting: GBR, SVR, LSTM, TCN\n",
        "\n",
        "\n",
        "- Data: historical OHLCV \n",
        "- Features: technical indicators and lag features\n",
        "- Validation: expanding-window walk-forward evaluation\n",
        "- Models: Gradient Boosting Regressor (GBR), Support Vector Regressor (SVR), LSTM, Temporal Convolutional Network (TCN)\n",
        "- Metrics: RMSE, MAE, MAPE, latency\n",
        "- Artifacts: saves trained models and a registry JSON for a Streamlit app\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. Run the install cell below; ensure it completes successfully.\n",
        "3. Optionally mount Google Drive or local download to retain artifacts.\n",
        "4. Execute the end-to-end cell to train across models and save artifacts into `artifacts/`.\n",
        "5. Download the `artifacts/` folder into this repository to use with the Streamlit app.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip -q install yfinance ta plotly scikit-learn tensorflow keras-tcn joblib pyyaml\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#  DATA COLLECTION & INITIAL INSPECTION ENHANCEMENTS \n",
        "\n",
        "def comprehensive_data_inspection(df, symbol):\n",
        "    \"\"\"Enhanced data inspection with statistical analysis and visualizations\"\"\"\n",
        "    print(f\"\\n DATA INSPECTION FOR {symbol} \")\n",
        "    # Basic info\n",
        "    print(f\"Dataset shape: {df.shape}\")\n",
        "    print(f\"Date range: {df.index.min()} to {df.index.max()}\")\n",
        "    print(f\"Total trading days: {len(df)}\")\n",
        "    # Missing values analysis\n",
        "    missing_analysis = df.isnull().sum()\n",
        "    if missing_analysis.sum() > 0:\n",
        "        print(f\"\\nMissing values:\\n{missing_analysis}\")\n",
        "    # Statistical summary\n",
        "    print(f\"\\nStatistical Summary:\")\n",
        "    print(df.describe())\n",
        "    # Price statistics\n",
        "    print(f\"\\nPrice Analysis:\")\n",
        "    print(f\"Min price: ${df['close'].min():.2f}\")\n",
        "    print(f\"Max price: ${df['close'].max():.2f}\")\n",
        "    print(f\"Average daily return: {df['close'].pct_change().mean()*100:.4f}%\")\n",
        "    print(f\"Volatility (std of returns): {df['close'].pct_change().std()*100:.4f}%\")\n",
        "    # Volume analysis\n",
        "    print(f\"\\nVolume Analysis:\")\n",
        "    print(f\"Average daily volume: {df['volume'].mean():,.0f}\")\n",
        "    print(f\"Volume coefficient of variation: {df['volume'].std()/df['volume'].mean():.2f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from plotly.subplots import make_subplots\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "def plot_data_quality_assessment(df, symbol, save_as=None):\n",
        "    \"\"\"Comprehensive data quality visualization\"\"\"\n",
        "    fig = make_subplots(\n",
        "        rows=3, cols=2,\n",
        "        subplot_titles=['Price Time Series', 'Volume Time Series', \n",
        "                       'Daily Returns Distribution', 'Volume Distribution',\n",
        "                       'Price vs Volume Correlation', 'Missing Values Heatmap'],\n",
        "        specs=[[{}, {}], [{}, {}], [{}, {}]]\n",
        "    )\n",
        "    # Price time series\n",
        "    fig.add_trace(go.Scatter(x=df.index, y=df['close'], name='Close Price'), row=1, col=1)\n",
        "    # Volume time series  \n",
        "    fig.add_trace(go.Scatter(x=df.index, y=df['volume'], name='Volume'), row=1, col=2)\n",
        "    # Returns distribution\n",
        "    returns = df['close'].pct_change().dropna()\n",
        "    fig.add_trace(go.Histogram(x=returns, name='Returns', nbinsx=50), row=2, col=1)\n",
        "    # Volume distribution\n",
        "    fig.add_trace(go.Histogram(x=df['volume'], name='Volume', nbinsx=50), row=2, col=2)\n",
        "    # Price vs Volume scatter\n",
        "    fig.add_trace(go.Scatter(x=df['volume'], y=df['close'], mode='markers', \n",
        "                           name='Price vs Volume', opacity=0.6), row=3, col=1)\n",
        "    fig.update_layout(title=f\"{symbol} - Data Quality Assessment\", height=900)\n",
        "    if save_as:\n",
        "        save_figure(fig, save_as)\n",
        "    return fig\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import math\n",
        "import time\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotly.graph_objects as go\n",
        "import yfinance as yf\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.svm import SVR\n",
        "\n",
        "import joblib\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Deep learning\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Optional: TCN\n",
        "try:\n",
        "    from tcn import TCN\n",
        "    TCN_AVAILABLE = True\n",
        "except Exception:\n",
        "    TCN_AVAILABLE = False\n",
        "\n",
        "# Optional: Refinitiv (import lazily in adapter)\n",
        "\n",
        "# Default config\n",
        "DEFAULT_SYMBOLS = [\"BTC-USD\", \"ETH-USD\"]\n",
        "DEFAULT_INTERVAL = \"1d\"  # options: 1d, 1h, 15m, etc.\n",
        "DEFAULT_START = \"2018-01-01\"\n",
        "DEFAULT_END = None  # until latest\n",
        "DEFAULT_LOOKBACK = 60\n",
        "DEFAULT_HORIZON = 1  # predict next close\n",
        "ARTIFACT_DIR = Path(\"artifacts\")\n",
        "ARTIFACT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "tf.random.set_seed(RANDOM_SEED)\n",
        "\n",
        "print(tf.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def set_global_seed(seed: int = 42) -> None:\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "\n",
        "\n",
        "def ensure_dir(path: Path) -> None:\n",
        "    path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "\n",
        "def rmse(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
        "    return float(np.sqrt(np.mean((y_true - y_pred) ** 2)))\n",
        "\n",
        "\n",
        "def mape(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
        "    y_true = np.array(y_true)\n",
        "    y_pred = np.array(y_pred)\n",
        "    denom = np.clip(np.abs(y_true), 1e-8, None)\n",
        "    return float(np.mean(np.abs((y_true - y_pred) / denom)) * 100.0)\n",
        "\n",
        "\n",
        "class Timer:\n",
        "    def __init__(self, label: str):\n",
        "        self.label = label\n",
        "        self.start_ts = None\n",
        "\n",
        "    def __enter__(self):\n",
        "        self.start_ts = time.time()\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc, tb):\n",
        "        elapsed = time.time() - self.start_ts\n",
        "        print(f\"{self.label} took {elapsed:.2f}s\")\n",
        "\n",
        "\n",
        "def mount_gdrive_if_colab(mount_point: str = \"/content/drive\") -> Optional[str]:\n",
        "    try:\n",
        "        import google.colab  # noqa: F401\n",
        "        from google.colab import drive\n",
        "        drive.mount(mount_point)\n",
        "        return mount_point\n",
        "    except Exception:\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data Adapters\n",
        "\n",
        "def fetch_ohlcv_yfinance(symbol: str,\n",
        "                         start: Optional[str] = DEFAULT_START,\n",
        "                         end: Optional[str] = DEFAULT_END,\n",
        "                         interval: str = DEFAULT_INTERVAL) -> pd.DataFrame:\n",
        "    \n",
        "\n",
        "    # Use Ticker method instead of download for more reliable results\n",
        "    ticker = yf.Ticker(symbol)\n",
        "    df = ticker.history(\n",
        "        start=start,\n",
        "        end=end,\n",
        "        interval=interval,\n",
        "        auto_adjust=False,\n",
        "        prepost=True\n",
        "    )\n",
        "\n",
        "    # Ticker.history returns clean column names\n",
        "    df = df.rename(columns={\n",
        "        'Open': 'open',\n",
        "        'High': 'high',\n",
        "        'Low': 'low',\n",
        "        'Close': 'close',\n",
        "        'Volume': 'volume'\n",
        "    })\n",
        "\n",
        "    # Convert each column individually \n",
        "    for col in ['open', 'high', 'low', 'close', 'volume']:\n",
        "        if col in df.columns:\n",
        "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "    # Clean and return\n",
        "    df = df.dropna(subset=['open', 'high', 'low', 'close'])\n",
        "    return df[['open', 'high', 'low', 'close', 'volume']]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def fetch_ohlcv_refinitiv(instrument: str,\n",
        "                          start: Optional[str] = DEFAULT_START,\n",
        "                          end: Optional[str] = DEFAULT_END,\n",
        "                          interval: str = DEFAULT_INTERVAL) -> pd.DataFrame:\n",
        "    \"\"\"Fetch OHLCV using Refinitiv Data Platform if credentials are available.\n",
        "\n",
        "    Requirements:\n",
        "    - pip install refinitiv-data\n",
        "    - Set environment variable RD_APP_KEY with your AppKey\n",
        "\n",
        "    \n",
        "    \"\"\"\n",
        "    try:\n",
        "        import refinitiv.data as rd\n",
        "        from refinitiv.data.content import historical_pricing as hp\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(\"Refinitiv SDK not available. Install 'refinitiv-data' and configure credentials.\") from e\n",
        "\n",
        "    app_key = os.environ.get(\"RD_APP_KEY\")\n",
        "    if not app_key:\n",
        "        raise RuntimeError(\"RD_APP_KEY env var not set for Refinitiv.\")\n",
        "\n",
        "    rd.open_session(rd.PlatformSession(app_key))\n",
        "    try:\n",
        "        # Map interval to Refinitiv granularity. Simplify: support daily.\n",
        "        if interval not in (\"1d\", \"1D\", \"daily\"):\n",
        "            raise NotImplementedError(\"Refinitiv adapter currently supports daily interval.\")\n",
        "        q = hp.Definition(instrument).close().fields([\"OPEN\",\"HIGH\",\"LOW\",\"CLOSE\",\"VOLUME\"])\n",
        "        if start:\n",
        "            q = q.start_date(start)\n",
        "        if end:\n",
        "            q = q.end_date(end)\n",
        "        rsp = q.get_data()\n",
        "        if rsp is None or rsp.data is None or rsp.data.empty:\n",
        "            raise ValueError(f\"No data from Refinitiv for {instrument}\")\n",
        "        df = rsp.data.copy()\n",
        "        # Normalize columns\n",
        "        df = df.rename(columns={\n",
        "            'OPEN':'open','HIGH':'high','LOW':'low','CLOSE':'close','VOLUME':'volume'\n",
        "        })\n",
        "        df.index = pd.to_datetime(df.index, utc=True).tz_convert(None)\n",
        "        for c in ['open','high','low','close','volume']:\n",
        "            df[c] = pd.to_numeric(df[c], errors='coerce')\n",
        "        df = df.dropna(subset=['open','high','low','close','volume'])\n",
        "        return df[['open','high','low','close','volume']]\n",
        "    finally:\n",
        "        rd.close_session()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Bitfinex Adapter\n",
        "import requests\n",
        "\n",
        "_BITFINEX_TIMEFRAME_MAP = {\n",
        "    \"1m\": \"1m\", \"5m\": \"5m\", \"15m\": \"15m\", \"30m\": \"30m\",\n",
        "    \"1h\": \"1h\", \"3h\": \"3h\", \"6h\": \"6h\", \"12h\": \"12h\",\n",
        "    \"1d\": \"1D\", \"1D\": \"1D\"\n",
        "}\n",
        "\n",
        "\n",
        "def _to_bitfinex_symbol(symbol: str) -> str:\n",
        "    # 'BTC-USD' -> 'tBTCUSD'; 'ETH-USD' -> 'tETHUSD'\n",
        "    s = symbol.replace(\"-\", \"\")\n",
        "    if not s.startswith(\"t\"):\n",
        "        s = \"t\" + s\n",
        "    return s\n",
        "\n",
        "\n",
        "def fetch_ohlcv_bitfinex(symbol: str,\n",
        "                          start: Optional[pd.Timestamp] = None,\n",
        "                          end: Optional[pd.Timestamp] = None,\n",
        "                          interval: str = DEFAULT_INTERVAL,\n",
        "                          limit: int = 1000,\n",
        "                          sort: int = 1) -> pd.DataFrame:\n",
        "    \"\"\"Fetch OHLCV from Bitfinex candles API.\n",
        "\n",
        "    interval examples: '1d','1h','15m' mapped to Bitfinex timeframes.\n",
        "    Returns columns: open, high, low, close, volume, indexed by timestamp.\n",
        "    \"\"\"\n",
        "    tf = _BITFINEX_TIMEFRAME_MAP.get(interval, \"1D\")\n",
        "    sym = _to_bitfinex_symbol(symbol)\n",
        "    base = f\"https://api-pub.bitfinex.com/v2/candles/trade:{tf}:{sym}/hist\"\n",
        "    params = {\"limit\": limit, \"sort\": sort}\n",
        "    if start is not None:\n",
        "        params[\"start\"] = int(pd.Timestamp(start).value // 10**6)\n",
        "    if end is not None:\n",
        "        params[\"end\"] = int(pd.Timestamp(end).value // 10**6)\n",
        "    headers = {\"accept\": \"application/json\"}\n",
        "    r = requests.get(base, params=params, headers=headers, timeout=30)\n",
        "    r.raise_for_status()\n",
        "    data = r.json()\n",
        "    if not data:\n",
        "        raise ValueError(f\"No data returned from Bitfinex for {symbol}\")\n",
        "    # Each row: [MTS, OPEN, CLOSE, HIGH, LOW, VOLUME]\n",
        "    arr = np.array(data)\n",
        "    # Ensure ascending by timestamp\n",
        "    arr = arr[np.argsort(arr[:, 0])]\n",
        "    mts = pd.to_datetime(arr[:, 0], unit='ms')\n",
        "    df = pd.DataFrame({\n",
        "        'open': arr[:, 1],\n",
        "        'close': arr[:, 2],\n",
        "        'high': arr[:, 3],\n",
        "        'low': arr[:, 4],\n",
        "        'volume': arr[:, 5]\n",
        "    }, index=mts)\n",
        "    df.index = pd.to_datetime(df.index, utc=True).tz_convert(None)\n",
        "    # Reorder to standard column order\n",
        "    df = df[['open','high','low','close','volume']]\n",
        "    df = df.apply(pd.to_numeric, errors='coerce').dropna()\n",
        "    return df\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature Engineering\n",
        "\n",
        "from pandas.api.types import is_numeric_dtype\n",
        "\n",
        "\n",
        "def compute_rsi(series: pd.Series, window: int = 14) -> pd.Series:\n",
        "    delta = series.diff()\n",
        "    gain = (delta.clip(lower=0)).rolling(window).mean()\n",
        "    loss = (-delta.clip(upper=0)).rolling(window).mean()\n",
        "    rs = gain / (loss + 1e-9)\n",
        "    rsi = 100 - (100 / (1 + rs))\n",
        "    return rsi\n",
        "\n",
        "\n",
        "def add_technical_indicators(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    out = df.copy()\n",
        "    out['ret'] = out['close'].pct_change()\n",
        "    out['log_ret'] = np.log(out['close']).diff()\n",
        "\n",
        "    out['sma_10'] = out['close'].rolling(10).mean()\n",
        "    out['sma_20'] = out['close'].rolling(20).mean()\n",
        "    out['ema_10'] = out['close'].ewm(span=10, adjust=False).mean()\n",
        "\n",
        "    out['rsi_14'] = compute_rsi(out['close'], 14)\n",
        "\n",
        "    out['bb_mid'] = out['close'].rolling(20).mean()\n",
        "    out['bb_std'] = out['close'].rolling(20).std()\n",
        "    out['bb_up'] = out['bb_mid'] + 2 * out['bb_std']\n",
        "    out['bb_low'] = out['bb_mid'] - 2 * out['bb_std']\n",
        "\n",
        "    out['volatility_20'] = out['log_ret'].rolling(20).std() * np.sqrt(20)\n",
        "\n",
        "    out = out.dropna()\n",
        "    # Ensure numeric dtypes\n",
        "    for col in out.columns:\n",
        "        if not is_numeric_dtype(out[col]):\n",
        "            out[col] = pd.to_numeric(out[col], errors='coerce')\n",
        "    out = out.dropna()\n",
        "    return out\n",
        "\n",
        "\n",
        "def select_feature_columns(df: pd.DataFrame) -> List[str]:\n",
        "    cols = [c for c in df.columns if c not in ['close']]\n",
        "    return cols\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## Windowing and splits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Windowing utilities\n",
        "\n",
        "def make_supervised_tabular(df: pd.DataFrame,\n",
        "                            feature_cols: List[str],\n",
        "                            target_col: str = 'close',\n",
        "                            lookback: int = DEFAULT_LOOKBACK,\n",
        "                            horizon: int = DEFAULT_HORIZON) -> Tuple[np.ndarray, np.ndarray, List[pd.Timestamp]]:\n",
        "    X_list, y_list, ts_list = [], [], []\n",
        "    values = df.copy()\n",
        "    for i in range(lookback, len(values) - horizon + 1):\n",
        "        X_window = values.iloc[i - lookback:i][feature_cols].values\n",
        "        X_list.append(X_window.reshape(-1))  # flatten window for tabular models\n",
        "        y_list.append(values.iloc[i + horizon - 1][target_col])\n",
        "        ts_list.append(values.index[i + horizon - 1])\n",
        "    return np.array(X_list), np.array(y_list), ts_list\n",
        "\n",
        "\n",
        "def make_sequence_tensor(df: pd.DataFrame,\n",
        "                         feature_cols: List[str],\n",
        "                         target_col: str = 'close',\n",
        "                         lookback: int = DEFAULT_LOOKBACK,\n",
        "                         horizon: int = DEFAULT_HORIZON) -> Tuple[np.ndarray, np.ndarray, List[pd.Timestamp]]:\n",
        "    X_list, y_list, ts_list = [], [], []\n",
        "    values = df.copy()\n",
        "    data = values[feature_cols].values\n",
        "    target = values[target_col].values\n",
        "    for i in range(lookback, len(values) - horizon + 1):\n",
        "        X_window = data[i - lookback:i]\n",
        "        X_list.append(X_window)\n",
        "        y_list.append(target[i + horizon - 1])\n",
        "        ts_list.append(values.index[i + horizon - 1])\n",
        "    return np.array(X_list), np.array(y_list), ts_list\n",
        "\n",
        "\n",
        "def expanding_window_splits(n_samples: int,\n",
        "                            n_splits: int = 5,\n",
        "                            min_train_size: int = 500,\n",
        "                            test_size: int = 100) -> List[Tuple[np.ndarray, np.ndarray]]:\n",
        "    \"\"\"Yield expanding train and rolling test indices.\"\"\"\n",
        "    splits = []\n",
        "    start = 0\n",
        "    train_end = max(min_train_size, int(n_samples * 0.6))\n",
        "    while train_end + test_size <= n_samples and len(splits) < n_splits:\n",
        "        train_idx = np.arange(start, train_end)\n",
        "        test_idx = np.arange(train_end, train_end + test_size)\n",
        "        splits.append((train_idx, test_idx))\n",
        "        train_end += test_size\n",
        "    return splits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model builders\n",
        "\n",
        "def build_gbr_pipeline(random_state: int = RANDOM_SEED) -> Pipeline:\n",
        "    return Pipeline([\n",
        "        (\"scaler\", StandardScaler(with_mean=True)),\n",
        "        (\"model\", GradientBoostingRegressor(random_state=random_state))\n",
        "    ])\n",
        "\n",
        "\n",
        "def build_svr_pipeline() -> Pipeline:\n",
        "    return Pipeline([\n",
        "        (\"scaler\", StandardScaler(with_mean=True)),\n",
        "        (\"model\", SVR(kernel='rbf', C=10.0, epsilon=0.1, gamma='scale'))\n",
        "    ])\n",
        "\n",
        "\n",
        "def build_lstm_model(input_steps: int, input_features: int) -> keras.Model:\n",
        "    inputs = keras.Input(shape=(input_steps, input_features))\n",
        "    x = layers.LSTM(64, return_sequences=True)(inputs)\n",
        "    x = layers.Dropout(0.2)(x)\n",
        "    x = layers.LSTM(32)(x)\n",
        "    x = layers.Dropout(0.2)(x)\n",
        "    x = layers.Dense(32, activation='relu')(x)\n",
        "    outputs = layers.Dense(1)(x)\n",
        "    model = keras.Model(inputs, outputs)\n",
        "    model.compile(optimizer=keras.optimizers.Adam(1e-3), loss='mse')\n",
        "    return model\n",
        "\n",
        "\n",
        "def build_tcn_model(input_steps: int, input_features: int) -> Optional[keras.Model]:\n",
        "    if not TCN_AVAILABLE:\n",
        "        return None\n",
        "    inputs = keras.Input(shape=(input_steps, input_features))\n",
        "    x = TCN(nb_filters=64, kernel_size=3, nb_stacks=2, dropout_rate=0.2, dilations=[1,2,4,8])(inputs)\n",
        "    x = layers.Dense(32, activation='relu')(x)\n",
        "    outputs = layers.Dense(1)(x)\n",
        "    model = keras.Model(inputs, outputs)\n",
        "    model.compile(optimizer=keras.optimizers.Adam(1e-3), loss='mse')\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training / evaluation helpers\n",
        "\n",
        "def evaluate_predictions(y_true: np.ndarray, y_pred: np.ndarray) -> Dict[str, float]:\n",
        "    return {\n",
        "        \"rmse\": rmse(y_true, y_pred),\n",
        "        \"mae\": float(mean_absolute_error(y_true, y_pred)),\n",
        "        \"mape\": mape(y_true, y_pred)\n",
        "    }\n",
        "\n",
        "\n",
        "def train_eval_sklearn(X: np.ndarray, y: np.ndarray, splits: List[Tuple[np.ndarray, np.ndarray]], pipeline: Pipeline) -> Dict:\n",
        "    metrics_list = []\n",
        "    preds_all, truth_all = [], []\n",
        "    for fold, (tr, te) in enumerate(splits, 1):\n",
        "        with Timer(f\"SKLearn fold {fold}\"):\n",
        "            pipeline.fit(X[tr], y[tr])\n",
        "            pred = pipeline.predict(X[te])\n",
        "        m = evaluate_predictions(y[te], pred)\n",
        "        metrics_list.append(m)\n",
        "        preds_all.append(pred)\n",
        "        truth_all.append(y[te])\n",
        "    agg = {k: float(np.mean([m[k] for m in metrics_list])) for k in metrics_list[0].keys()}\n",
        "    return {\"fold_metrics\": metrics_list, \"agg\": agg, \"preds\": np.concatenate(preds_all), \"truth\": np.concatenate(truth_all)}\n",
        "\n",
        "\n",
        "def train_eval_keras_seq(X: np.ndarray, y: np.ndarray, splits: List[Tuple[np.ndarray, np.ndarray]], build_model_fn, epochs: int = 20, batch_size: int = 64) -> Dict:\n",
        "    metrics_list = []\n",
        "    preds_all, truth_all = [], []\n",
        "    for fold, (tr, te) in enumerate(splits, 1):\n",
        "        with Timer(f\"Keras fold {fold}\"):\n",
        "            model = build_model_fn(X.shape[1], X.shape[2])\n",
        "            early = keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True, monitor='val_loss')\n",
        "            history = model.fit(\n",
        "                X[tr], y[tr],\n",
        "                validation_data=(X[te], y[te]),\n",
        "                epochs=epochs,\n",
        "                batch_size=batch_size,\n",
        "                verbose=0,\n",
        "                callbacks=[early]\n",
        "            )\n",
        "            pred = model.predict(X[te], verbose=0).ravel()\n",
        "        m = evaluate_predictions(y[te], pred)\n",
        "        metrics_list.append(m)\n",
        "        preds_all.append(pred)\n",
        "        truth_all.append(y[te])\n",
        "    agg = {k: float(np.mean([m[k] for m in metrics_list])) for k in metrics_list[0].keys()}\n",
        "    return {\"fold_metrics\": metrics_list, \"agg\": agg, \"preds\": np.concatenate(preds_all), \"truth\": np.concatenate(truth_all)}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Artifact saving and registry\n",
        "\n",
        "REGISTRY_PATH = ARTIFACT_DIR / \"registry.json\"\n",
        "\n",
        "def save_registry(registry: Dict) -> None:\n",
        "    with open(REGISTRY_PATH, 'w') as f:\n",
        "        json.dump(registry, f, indent=2)\n",
        "\n",
        "\n",
        "def load_registry() -> Dict:\n",
        "    if REGISTRY_PATH.exists():\n",
        "        with open(REGISTRY_PATH, 'r') as f:\n",
        "            return json.load(f)\n",
        "    return {\"models\": []}\n",
        "\n",
        "\n",
        "def register_model_entry(registry: Dict, entry: Dict) -> Dict:\n",
        "    reg = load_registry() if registry is None else registry\n",
        "    reg.setdefault(\"models\", []).append(entry)\n",
        "    save_registry(reg)\n",
        "    return reg\n",
        "\n",
        "\n",
        "def save_sklearn_model(pipeline: Pipeline, model_name: str, symbol: str, lookback: int, horizon: int) -> Dict:\n",
        "    ensure_dir(ARTIFACT_DIR)\n",
        "    path = ARTIFACT_DIR / f\"{model_name}_{symbol}_lb{lookback}_h{horizon}.joblib\"\n",
        "    joblib.dump(pipeline, path)\n",
        "    entry = {\n",
        "        \"type\": \"sklearn\",\n",
        "        \"name\": model_name,\n",
        "        \"symbol\": symbol,\n",
        "        \"lookback\": lookback,\n",
        "        \"horizon\": horizon,\n",
        "        \"path\": str(path)\n",
        "    }\n",
        "    return entry\n",
        "\n",
        "\n",
        "def save_keras_model(model: keras.Model, model_name: str, symbol: str, lookback: int, horizon: int, scaler_x: Optional[MinMaxScaler] = None, scaler_y: Optional[MinMaxScaler] = None) -> Dict:\n",
        "    ensure_dir(ARTIFACT_DIR)\n",
        "    model_path = ARTIFACT_DIR / f\"{model_name}_{symbol}_lb{lookback}_h{horizon}.keras\"\n",
        "    model.save(model_path)\n",
        "    entry = {\n",
        "        \"type\": \"keras\",\n",
        "        \"name\": model_name,\n",
        "        \"symbol\": symbol,\n",
        "        \"lookback\": lookback,\n",
        "        \"horizon\": horizon,\n",
        "        \"path\": str(model_path),\n",
        "        \"scaler_x\": None,\n",
        "        \"scaler_y\": None\n",
        "    }\n",
        "    if scaler_x is not None:\n",
        "        sx_path = ARTIFACT_DIR / f\"{model_name}_{symbol}_lb{lookback}_h{horizon}_scaler_x.joblib\"\n",
        "        joblib.dump(scaler_x, sx_path)\n",
        "        entry[\"scaler_x\"] = str(sx_path)\n",
        "    if scaler_y is not None:\n",
        "        sy_path = ARTIFACT_DIR / f\"{model_name}_{symbol}_lb{lookback}_h{horizon}_scaler_y.joblib\"\n",
        "        joblib.dump(scaler_y, sy_path)\n",
        "        entry[\"scaler_y\"] = str(sy_path)\n",
        "    return entry\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use Case \n",
        "SYMBOL = \"BTC-USD\"\n",
        "INTERVAL = \"1d\"\n",
        "\n",
        "df_bfx = fetch_ohlcv_bitfinex(SYMBOL, interval=INTERVAL, limit=5000)\n",
        "df_feat_bfx = add_technical_indicators(df_bfx)\n",
        "features_bfx = select_feature_columns(df_feat_bfx)\n",
        "\n",
        "X_tab_bfx, y_tab_bfx, ts_tab_bfx = make_supervised_tabular(df_feat_bfx, features_bfx, 'close', DEFAULT_LOOKBACK, DEFAULT_HORIZON)\n",
        "splits_bfx = expanding_window_splits(len(X_tab_bfx), n_splits=3, min_train_size=500, test_size=100)\n",
        "\n",
        "res_gbr_bfx = train_eval_sklearn(X_tab_bfx, y_tab_bfx, splits_bfx, build_gbr_pipeline())\n",
        "print(\"Bitfinex GBR agg:\", res_gbr_bfx[\"agg\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# End-to-end example run\n",
        "\n",
        "SYMBOL = \"BTC-USD\"\n",
        "INTERVAL = DEFAULT_INTERVAL\n",
        "START = DEFAULT_START\n",
        "END = DEFAULT_END\n",
        "LOOKBACK = 60\n",
        "HORIZON = 1\n",
        "\n",
        "# Fetch data \n",
        "df_raw = fetch_ohlcv_yfinance(SYMBOL, start=START, end=END, interval=INTERVAL)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "#  ENHANCED DATA INSPECTION \n",
        "comprehensive_data_inspection(df_raw, SYMBOL)\n",
        "_ = plot_data_quality_assessment(df_raw, SYMBOL, save_as=f\"{SYMBOL}_data_quality\")\n",
        "\n",
        "df_feat = add_technical_indicators(df_raw)\n",
        "features = select_feature_columns(df_feat)\n",
        "\n",
        "# Prepare supervised datasets\n",
        "X_tab, y_tab, ts_tab = make_supervised_tabular(df_feat, features, 'close', LOOKBACK, HORIZON)\n",
        "X_seq, y_seq, ts_seq = make_sequence_tensor(df_feat, features, 'close', LOOKBACK, HORIZON)\n",
        "\n",
        "# Create splits\n",
        "splits = expanding_window_splits(len(X_tab), n_splits=5, min_train_size=500, test_size=100)\n",
        "\n",
        "results_summary = {}\n",
        "registry = load_registry()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GBR\n",
        "gbr = build_gbr_pipeline()\n",
        "res_gbr = train_eval_sklearn(X_tab, y_tab, splits, gbr)\n",
        "print(\"GBR agg:\", res_gbr[\"agg\"]) \n",
        "entry = save_sklearn_model(gbr, \"GBR\", SYMBOL, LOOKBACK, HORIZON)\n",
        "registry = register_model_entry(registry, entry)\n",
        "results_summary[\"GBR\"] = res_gbr[\"agg\"]\n",
        "\n",
        "# SVR\n",
        "svr = build_svr_pipeline()\n",
        "res_svr = train_eval_sklearn(X_tab, y_tab, splits, svr)\n",
        "print(\"SVR agg:\", res_svr[\"agg\"]) \n",
        "entry = save_sklearn_model(svr, \"SVR\", SYMBOL, LOOKBACK, HORIZON)\n",
        "registry = register_model_entry(registry, entry)\n",
        "results_summary[\"SVR\"] = res_svr[\"agg\"]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# LSTM\n",
        "scaler_x = MinMaxScaler()\n",
        "scaler_y = MinMaxScaler()\n",
        "X_seq_scaled = scaler_x.fit_transform(X_seq.reshape(X_seq.shape[0], -1)).reshape(X_seq.shape)\n",
        "y_seq_scaled = scaler_y.fit_transform(y_seq.reshape(-1, 1)).reshape(-1)\n",
        "\n",
        "lstm_model = build_lstm_model(X_seq.shape[1], X_seq.shape[2])\n",
        "res_lstm = train_eval_keras_seq(X_seq_scaled, y_seq_scaled, splits, lambda steps, feats: build_lstm_model(steps, feats))\n",
        "print(\"LSTM agg (on scaled):\", res_lstm[\"agg\"]) \n",
        "entry = save_keras_model(lstm_model, \"LSTM\", SYMBOL, LOOKBACK, HORIZON, scaler_x, scaler_y)\n",
        "registry = register_model_entry(registry, entry)\n",
        "results_summary[\"LSTM_scaled\"] = res_lstm[\"agg\"]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TCN (optional)\n",
        "if TCN_AVAILABLE:\n",
        "    tcn_model = build_tcn_model(X_seq.shape[1], X_seq.shape[2])\n",
        "    res_tcn = train_eval_keras_seq(X_seq_scaled, y_seq_scaled, splits, lambda steps, feats: build_tcn_model(steps, feats))\n",
        "    print(\"TCN agg (on scaled):\", res_tcn[\"agg\"]) \n",
        "    entry = save_keras_model(tcn_model, \"TCN\", SYMBOL, LOOKBACK, HORIZON, scaler_x, scaler_y)\n",
        "    registry = register_model_entry(registry, entry)\n",
        "    results_summary[\"TCN_scaled\"] = res_tcn[\"agg\"]\n",
        "else:\n",
        "    print(\"TCN not available. Install keras-tcn to enable.\")\n",
        "\n",
        "print(\"Summary:\", json.dumps(results_summary, indent=2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualization Utilities "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualization utilities\n",
        "import math\n",
        "from typing import Iterable\n",
        "import plotly.express as px\n",
        "import plotly.io as pio\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "\n",
        "def save_figure(fig, name: str, formats: Iterable[str] = (\"html\",)) -> None:\n",
        "    \"\"\"Save Plotly figure under artifacts/figures.\n",
        "    formats can include 'html' and/or 'png' (requires kaleido for PNG).\n",
        "    \"\"\"\n",
        "    fig_dir = ARTIFACT_DIR / \"figures\"\n",
        "    ensure_dir(fig_dir)\n",
        "    if \"html\" in formats:\n",
        "        fig.write_html(fig_dir / f\"{name}.html\")\n",
        "    if \"png\" in formats:\n",
        "        try:\n",
        "            pio.write_image(fig, fig_dir / f\"{name}.png\", scale=2, width=1280, height=720)\n",
        "        except Exception as e:\n",
        "            print(f\"PNG export skipped (install kaleido to enable): {e}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def plot_candlestick(df: pd.DataFrame, title: str = \"Candlestick\", save_as: str = None):\n",
        "    df = df.copy()\n",
        "    fig = make_subplots(rows=2, cols=1, shared_xaxes=True, vertical_spacing=0.03,\n",
        "                        row_heights=[0.7, 0.3])\n",
        "    fig.add_trace(go.Candlestick(x=df.index, open=df['open'], high=df['high'], low=df['low'], close=df['close'],\n",
        "                                 name='OHLC'), row=1, col=1)\n",
        "    if 'volume' in df.columns:\n",
        "        fig.add_trace(go.Bar(x=df.index, y=df['volume'], name='Volume'), row=2, col=1)\n",
        "    fig.update_layout(title=title, xaxis_rangeslider_visible=False)\n",
        "    if save_as:\n",
        "        save_figure(fig, save_as)\n",
        "    return fig\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def plot_indicator_overlays(df: pd.DataFrame, overlays: Iterable[str], title: str = \"Indicators\", save_as: str = None):\n",
        "    df = df.copy()\n",
        "    fig = make_subplots(rows=2, cols=1, shared_xaxes=True, vertical_spacing=0.04,\n",
        "                        row_heights=[0.7, 0.3], specs=[[{}],[{}]])\n",
        "    fig.add_trace(go.Scatter(x=df.index, y=df['close'], name='Close'), row=1, col=1)\n",
        "    for col in overlays:\n",
        "        if col in df.columns:\n",
        "            fig.add_trace(go.Scatter(x=df.index, y=df[col], name=col), row=1, col=1)\n",
        "    if 'rsi_14' in df.columns:\n",
        "        fig.add_trace(go.Scatter(x=df.index, y=df['rsi_14'], name='RSI 14'), row=2, col=1)\n",
        "        fig.add_hline(y=30, line=dict(color='gray', dash='dot'), row=2, col=1)\n",
        "        fig.add_hline(y=70, line=dict(color='gray', dash='dot'), row=2, col=1)\n",
        "    fig.update_layout(title=title)\n",
        "    if save_as:\n",
        "        save_figure(fig, save_as)\n",
        "    return fig\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_returns_and_volatility(df: pd.DataFrame, title: str = \"Returns & Volatility\", save_as: str = None):\n",
        "    df = df.copy()\n",
        "    fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
        "    if 'ret' in df.columns:\n",
        "        fig.add_trace(go.Scatter(x=df.index, y=df['ret'], name='Return'), secondary_y=False)\n",
        "    if 'volatility_20' in df.columns:\n",
        "        fig.add_trace(go.Scatter(x=df.index, y=df['volatility_20'], name='Vol 20'), secondary_y=True)\n",
        "    fig.update_layout(title=title)\n",
        "    if save_as:\n",
        "        save_figure(fig, save_as)\n",
        "    return fig\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_corr_heatmap(df_features: pd.DataFrame, title: str = \"Feature Correlation\", save_as: str = None):\n",
        "    corr = df_features.corr(numeric_only=True)\n",
        "    fig = go.Figure(data=go.Heatmap(z=corr.values, x=corr.columns, y=corr.columns, colorscale='Viridis'))\n",
        "    fig.update_layout(title=title)\n",
        "    if save_as:\n",
        "        save_figure(fig, save_as)\n",
        "    return fig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def plot_feature_distributions(df_features: pd.DataFrame, columns: Iterable[str] = None, title: str = \"Feature Distributions\", save_as: str = None):\n",
        "    cols = list(columns) if columns else list(df_features.columns)\n",
        "    n = len(cols)\n",
        "    rows = math.ceil(n / 2)\n",
        "    fig = make_subplots(rows=rows, cols=2, subplot_titles=cols)\n",
        "    r = c = 1\n",
        "    for col in cols:\n",
        "        if col not in df_features.columns:\n",
        "            continue\n",
        "        fig.add_trace(go.Histogram(x=df_features[col], name=col, nbinsx=50, opacity=0.8), row=r, col=c)\n",
        "        c = 2 if c == 1 else 1\n",
        "        if c == 1:\n",
        "            r += 1\n",
        "    fig.update_layout(title=title, barmode='overlay')\n",
        "    if save_as:\n",
        "        save_figure(fig, save_as)\n",
        "    return fig\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def align_predictions_with_timestamps(ts_list: list, splits: list, y_true_concat: np.ndarray, y_pred_concat: np.ndarray) -> pd.DataFrame:\n",
        "    \"\"\"Align concatenated preds/truth to timestamps using the provided splits.\n",
        "    Returns DataFrame with columns: actual, predicted, residual, fold.\n",
        "    \"\"\"\n",
        "    test_ts_all = []\n",
        "    fold_ids = []\n",
        "    for fold_id, (_, te) in enumerate(splits, 1):\n",
        "        test_ts_all.extend([ts_list[i] for i in te])\n",
        "        fold_ids.extend([fold_id] * len(te))\n",
        "    df = pd.DataFrame({\n",
        "        'ts': pd.to_datetime(test_ts_all),\n",
        "        'actual': y_true_concat,\n",
        "        'predicted': y_pred_concat,\n",
        "        'fold': fold_ids\n",
        "    }).sort_values('ts')\n",
        "    df['residual'] = df['actual'] - df['predicted']\n",
        "    df = df.set_index('ts')\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_walkforward_predictions(ts_list, y_true_concat, y_pred_concat, splits, title: str = \"Walk-forward\", save_as: str = None):\n",
        "    dfp = align_predictions_with_timestamps(ts_list, splits, y_true_concat, y_pred_concat)\n",
        "    fig = go.Figure()\n",
        "    fig.add_trace(go.Scatter(x=dfp.index, y=dfp['actual'], name='Actual'))\n",
        "    fig.add_trace(go.Scatter(x=dfp.index, y=dfp['predicted'], name='Predicted'))\n",
        "    # Shade test windows\n",
        "    for (_, te) in splits:\n",
        "        x0 = ts_list[te[0]]\n",
        "        x1 = ts_list[te[-1]]\n",
        "        fig.add_vrect(x0=x0, x1=x1, fillcolor=\"LightSalmon\", opacity=0.15, layer=\"below\", line_width=0)\n",
        "    fig.update_layout(title=title)\n",
        "    if save_as:\n",
        "        save_figure(fig, save_as)\n",
        "    return fig\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_residuals_over_time(ts_list, y_true_concat, y_pred_concat, splits, title: str = \"Residuals\", save_as: str = None):\n",
        "    dfp = align_predictions_with_timestamps(ts_list, splits, y_true_concat, y_pred_concat)\n",
        "    fig = go.Figure()\n",
        "    fig.add_trace(go.Scatter(x=dfp.index, y=dfp['residual'], name='Residual'))\n",
        "    if len(dfp) > 20:\n",
        "        fig.add_trace(go.Scatter(x=dfp.index, y=dfp['residual'].rolling(20).mean(), name='Rolling mean (20)'))\n",
        "    fig.update_layout(title=title)\n",
        "    if save_as:\n",
        "        save_figure(fig, save_as)\n",
        "    return fig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def plot_error_distribution(y_true_concat, y_pred_concat, title: str = \"Error Distribution\", save_as: str = None):\n",
        "    resid = (y_true_concat - y_pred_concat).ravel()\n",
        "    fig = go.Figure()\n",
        "    fig.add_trace(go.Histogram(x=resid, nbinsx=50, name='Residuals', opacity=0.85))\n",
        "    fig.add_vline(x=float(np.mean(resid)), line_dash='dash', line_color='red', annotation_text='mean', annotation_position='top')\n",
        "    fig.update_layout(title=title)\n",
        "    if save_as:\n",
        "        save_figure(fig, save_as)\n",
        "    return fig\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def plot_parity(y_true_concat, y_pred_concat, title: str = \"Parity Plot\", save_as: str = None):\n",
        "    fig = go.Figure()\n",
        "    fig.add_trace(go.Scatter(x=y_true_concat, y=y_pred_concat, mode='markers', name='Points', opacity=0.6))\n",
        "    mn = float(np.min([np.min(y_true_concat), np.min(y_pred_concat)]))\n",
        "    mx = float(np.max([np.max(y_true_concat), np.max(y_pred_concat)]))\n",
        "    fig.add_trace(go.Scatter(x=[mn, mx], y=[mn, mx], mode='lines', name='y=x'))\n",
        "    fig.update_layout(title=title, xaxis_title='Actual', yaxis_title='Predicted')\n",
        "    if save_as:\n",
        "        save_figure(fig, save_as)\n",
        "    return fig\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_model_comparison_bar(metrics_by_model: Dict[str, Dict[str, float]], metric: str = 'rmse', title: str = None, save_as: str = None):\n",
        "    names = []\n",
        "    values = []\n",
        "    for k, v in metrics_by_model.items():\n",
        "        if v is None or metric not in v:\n",
        "            continue\n",
        "        names.append(k)\n",
        "        values.append(v[metric])\n",
        "    fig = go.Figure(go.Bar(x=names, y=values))\n",
        "    fig.update_layout(title=title or f\"Model comparison by {metric.upper()}\", xaxis_title='Model', yaxis_title=metric.upper())\n",
        "    if save_as:\n",
        "        save_figure(fig, save_as)\n",
        "    return fig\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate visualizations\n",
        "_ = plot_candlestick(df_feat.tail(200), title=f\"{SYMBOL} - last 200 bars\", save_as=f\"{SYMBOL}_candles\")\n",
        "_ = plot_indicator_overlays(df_feat.tail(200), overlays=['sma_10','sma_20','ema_10','bb_up','bb_low','rsi_14'],\n",
        "                            title=f\"{SYMBOL} indicators\", save_as=f\"{SYMBOL}_indicators\")\n",
        "_ = plot_returns_and_volatility(df_feat, title=f\"{SYMBOL} returns & volatility\", save_as=f\"{SYMBOL}_returns_vol\")\n",
        "_ = plot_corr_heatmap(df_feat[select_feature_columns(df_feat)], title=\"Feature correlation\", save_as=\"features_corr\")\n",
        "\n",
        "cols_for_dist = select_feature_columns(df_feat)\n",
        "if len(df_feat) > 2000:\n",
        "    sample_df = df_feat[cols_for_dist].sample(2000, replace=False, random_state=RANDOM_SEED)\n",
        "else:\n",
        "    sample_df = df_feat[cols_for_dist]\n",
        "_ = plot_feature_distributions(sample_df, columns=cols_for_dist[:8], title=\"Feature distributions (subset)\", save_as=\"features_dists\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# 2) Model comparison bar\n",
        "metrics_by_model = {\n",
        "    \"GBR\": results_summary.get(\"GBR\"),\n",
        "    \"SVR\": results_summary.get(\"SVR\"),\n",
        "    \"LSTM (scaled)\": results_summary.get(\"LSTM_scaled\"),\n",
        "}\n",
        "if TCN_AVAILABLE and \"TCN_scaled\" in results_summary:\n",
        "    metrics_by_model[\"TCN (scaled)\"] = results_summary.get(\"TCN_scaled\")\n",
        "_ = plot_model_comparison_bar(metrics_by_model, metric=\"rmse\", title=\"Model comparison (RMSE)\", save_as=\"model_comp_rmse\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3) Model Evaluation plots\n",
        "# GBR walk-forward\n",
        "_ = plot_walkforward_predictions(ts_tab, res_gbr[\"truth\"], res_gbr[\"preds\"], splits, title=\"GBR walk-forward\", save_as=\"gbr_walk\")\n",
        "_ = plot_residuals_over_time(ts_tab, res_gbr[\"truth\"], res_gbr[\"preds\"], splits, title=\"GBR residuals\", save_as=\"gbr_residuals\")\n",
        "_ = plot_error_distribution(res_gbr[\"truth\"], res_gbr[\"preds\"], title=\"GBR error distribution\", save_as=\"gbr_errdist\")\n",
        "_ = plot_parity(res_gbr[\"truth\"], res_gbr[\"preds\"], title=\"GBR parity\", save_as=\"gbr_parity\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# LSTM (scaled)\n",
        "try:\n",
        "    def invert_scaled(arr, scaler):\n",
        "        return scaler.inverse_transform(np.array(arr).reshape(-1,1)).ravel()\n",
        "    y_true_lstm = invert_scaled(res_lstm[\"truth\"], scaler_y)\n",
        "    y_pred_lstm = invert_scaled(res_lstm[\"preds\"], scaler_y)\n",
        "    _ = plot_walkforward_predictions(ts_seq, y_true_lstm, y_pred_lstm, splits, title=\"LSTM walk-forward (price)\", save_as=\"lstm_walk\")\n",
        "    _ = plot_residuals_over_time(ts_seq, y_true_lstm, y_pred_lstm, splits, title=\"LSTM residuals (price)\", save_as=\"lstm_residuals\")\n",
        "    _ = plot_error_distribution(y_true_lstm, y_pred_lstm, title=\"LSTM error distribution (price)\", save_as=\"lstm_errdist\")\n",
        "    _ = plot_parity(y_true_lstm, y_pred_lstm, title=\"LSTM parity (price)\", save_as=\"lstm_parity\")\n",
        "except Exception as e:\n",
        "    print(\"Skipping LSTM price plots (no scaler or variables not in scope):\", e)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
